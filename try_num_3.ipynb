{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gey/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny-toxicity and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 312]) in the checkpoint and torch.Size([2, 312]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем модель и токенизатор\n",
    "model_name = \"cointegrated/rubert-tiny-toxicity\"\n",
    "num_labels = 2  # Бинарная классификация: токсичный/не токсичный\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Добавляем параметр ignore_mismatched_sizes=True\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True  # Игнорируем несоответствие размеров\n",
    ")\n",
    "\n",
    "# # Замораживаем базовые слои\n",
    "# for param in model.base_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/labeled.csv')\n",
    "# Преобразование столбца float_column в int\n",
    "df[\"toxic\"] = df[\"toxic\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11529/11529 [00:00<00:00, 11679.95 examples/s]\n",
      "Map: 100%|██████████| 2883/2883 [00:00<00:00, 10642.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделяем данные на train и val\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"comment\"].tolist(), df[\"toxic\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Создаем датасеты\n",
    "train_dataset = Dataset.from_dict({\"comment\": train_texts, \"toxic\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"comment\": val_texts, \"toxic\": val_labels})\n",
    "\n",
    "# Применяем токенизацию\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Удаляем столбец text, оставляем только токены и метки\n",
    "train_dataset = train_dataset.remove_columns([\"comment\"])\n",
    "val_dataset = val_dataset.remove_columns([\"comment\"])\n",
    "\n",
    "# Преобразуем метки в формат PyTorch\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny-toxicity and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 312]) in the checkpoint and torch.Size([2, 312]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gey/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training epoch 1:   0%|          | 0/361 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ToxicityTrainer(model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, val_dataset\u001b[38;5;241m=\u001b[39mval_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Запуск обучения\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Сохраняем модель\u001b[39;00m\n\u001b[1;32m    124\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxicity_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 53\u001b[0m, in \u001b[0;36mToxicityTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 53\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Преобразуем метки в тип float для BCEWithLogitsLoss\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Обнуляем градиенты\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=2):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Загружаем модель\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        # Замораживаем все слои, кроме последнего\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Меняем последний слой (linear)\n",
    "        self.model.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "class ToxicityTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, batch_size=16, lr=2e-5, epochs=3):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Даталоадеры для тренировочных и валидационных данных\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        # Оптимизатор\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Функция потерь для бинарной классификации\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Обучаем модель\n",
    "            for batch in tqdm(self.train_loader, desc=f\"Training epoch {epoch+1}\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                labels = batch['labels'].to(device).float()  # Преобразуем метки в тип float для BCEWithLogitsLoss\n",
    "\n",
    "                # Обнуляем градиенты\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Пропускаем данные через модель\n",
    "                outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Рассчитываем потерю\n",
    "                loss = self.loss_fn(logits.view(-1), labels.view(-1))  # Вычисляем loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = running_loss / len(self.train_loader)\n",
    "            print(f\"Epoch {epoch+1} - Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Проводим валидацию\n",
    "            self.validate()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                preds = torch.sigmoid(logits).cpu().numpy()  # Для бинарной классификации применяем sigmoid\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Применяем порог для бинарной классификации (0.5)\n",
    "        all_preds = [1 if pred >= 0.5 else 0 for pred in all_preds]\n",
    "        \n",
    "        # Рассчитываем метрики\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Инициализация\n",
    "model_name = \"cointegrated/rubert-tiny-toxicity\"  # Модель\n",
    "num_labels = 2  # Бинарная классификация\n",
    "\n",
    "# Загружаем токенизатор и датасеты\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Датасеты, которые вы подготовили для обучения (train_dataset, val_dataset)\n",
    "\n",
    "# Создаем модель\n",
    "model = CustomModel(base_model_name=model_name, num_labels=num_labels)\n",
    "\n",
    "# Создаем класс тренера\n",
    "trainer = ToxicityTrainer(model=model, train_dataset=train_dataset, val_dataset=val_dataset, epochs=5, batch_size=32)\n",
    "\n",
    "# Запуск обучения\n",
    "trainer.train()\n",
    "\n",
    "# Сохраняем модель\n",
    "trainer.save_model(\"toxicity_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
